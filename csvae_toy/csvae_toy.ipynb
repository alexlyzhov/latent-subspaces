{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import torch.utils.data as utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVAE(nn.Module):\n",
    "    def __init__(self, input_dim, labels_dim, z_dim, w_dim):\n",
    "        super(CSVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.labels_dim = labels_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.w_dim = w_dim\n",
    "        \n",
    "        self.encoder_xy_to_w = nn.Sequential(nn.Linear(input_dim+labels_dim, w_dim), nn.ReLU(), nn.Linear(w_dim, w_dim), nn.ReLU())\n",
    "        self.mu_xy_to_w = nn.Linear(w_dim, w_dim)\n",
    "        self.logvar_xy_to_w = nn.Linear(w_dim, w_dim)\n",
    "        \n",
    "        self.encoder_x_to_z = nn.Sequential(nn.Linear(input_dim, z_dim), nn.ReLU(), nn.Linear(z_dim, z_dim), nn.ReLU())\n",
    "        self.mu_x_to_z = nn.Linear(z_dim, z_dim)\n",
    "        self.logvar_x_to_z = nn.Linear(z_dim, z_dim)\n",
    "        \n",
    "        self.encoder_y_to_w = nn.Sequential(nn.Linear(labels_dim, w_dim), nn.ReLU(), nn.Linear(w_dim, w_dim), nn.ReLU())\n",
    "        self.mu_y_to_w = nn.Linear(w_dim, w_dim)\n",
    "        self.logvar_y_to_w = nn.Linear(w_dim, w_dim)\n",
    "        \n",
    "        # Add sigmoid or smth for images!\n",
    "        self.decoder_zw_to_x = nn.Sequential(nn.Linear(z_dim+w_dim, z_dim+w_dim), nn.ReLU(), nn.Linear(z_dim+w_dim, z_dim+w_dim), nn.ReLU())\n",
    "        self.mu_zw_to_x = nn.Linear(z_dim+w_dim, input_dim)\n",
    "        self.logvar_zw_to_x = nn.Linear(z_dim+w_dim, input_dim)\n",
    "        \n",
    "        self.decoder_z_to_y = nn.Sequential(nn.Linear(z_dim, z_dim), nn.ReLU(), nn.Linear(z_dim, z_dim), nn.ReLU(),\n",
    "                                            nn.Linear(z_dim, labels_dim), nn.Sigmoid())\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "        \n",
    "    def q_zw(self, x, y):\n",
    "        \"\"\"\n",
    "        VARIATIONAL POSTERIOR\n",
    "        :param x: input image\n",
    "        :return: parameters of q(z|x), (MB, hid_dim)\n",
    "        \"\"\"\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        intermediate = self.encoder_x_to_z(x)\n",
    "        z_mu = self.mu_x_to_z(intermediate)\n",
    "        z_logvar = self.mu_x_to_z(intermediate)\n",
    "        \n",
    "        intermediate = self.encoder_xy_to_w(xy)\n",
    "        w_mu_encoder = self.mu_xy_to_w(intermediate)\n",
    "        w_logvar_encoder = self.mu_xy_to_w(intermediate)\n",
    "        \n",
    "        intermediate = self.encoder_y_to_w(y)\n",
    "        w_mu_prior = self.mu_y_to_w(intermediate)\n",
    "        w_logvar_prior = self.mu_y_to_w(intermediate)\n",
    "        \n",
    "        return w_mu_encoder, w_logvar_encoder, w_mu_prior, \\\n",
    "               w_logvar_prior, z_mu, z_logvar\n",
    "    \n",
    "    def p_x(self, z, w):\n",
    "        \"\"\"\n",
    "        GENERATIVE DISTRIBUTION\n",
    "        :param z: latent vector          (MB, hid_dim)\n",
    "        :return: parameters of p(x|z)    (MB, inp_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        zw = torch.cat([z, w], dim=1)\n",
    "        \n",
    "        intermediate = self.decoder_zw_to_x(zw)\n",
    "        mu = self.mu_zw_to_x(intermediate)\n",
    "        logvar = self.logvar_zw_to_x(intermediate)\n",
    "        \n",
    "        return mu, logvar\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Encode the image, sample z and decode \n",
    "        :param x: input image\n",
    "        :return: parameters of p(x|z_hat), z_hat, parameters of q(z|x)\n",
    "        \"\"\"\n",
    "        w_mu_encoder, w_logvar_encoder, w_mu_prior, \\\n",
    "            w_logvar_prior, z_mu, z_logvar = self.q_zw(x, y)\n",
    "        w_encoder = self.reparameterize(w_mu_encoder, w_logvar_encoder)\n",
    "        w_prior = self.reparameterize(w_mu_prior, w_logvar_prior)\n",
    "        z = self.reparameterize(z_mu, z_logvar)\n",
    "        zw = torch.cat([z, w_encoder], dim=1)\n",
    "        \n",
    "        x_mu, x_logvar = self.p_x(z, w_encoder)\n",
    "        y_pred = self.decoder_z_to_y(z)\n",
    "        \n",
    "        return x_mu, x_logvar, zw, y_pred, \\\n",
    "               w_mu_encoder, w_logvar_encoder, w_mu_prior, \\\n",
    "               w_logvar_prior, z_mu, z_logvar\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Given the input batch, compute the negative ELBO \n",
    "        :param x:   (MB, inp_dim)\n",
    "        :param beta: Float\n",
    "        :param average: Compute average over mini batch or not, bool\n",
    "        :return: -RE + beta * KL  (MB, ) or (1, )\n",
    "        \"\"\"\n",
    "        x_mu, x_logvar, zw, y_pred, \\\n",
    "            w_mu_encoder, w_logvar_encoder, w_mu_prior, \\\n",
    "            w_logvar_prior, z_mu, z_logvar = self.forward(x, y)\n",
    "        \n",
    "        x_recon = nn.MSELoss()(x_mu, x)\n",
    "        \n",
    "        w_dist = dists.MultivariateNormal(w_mu_encoder.flatten(), torch.diag(w_logvar_encoder.flatten().exp()))\n",
    "        w_prior = dists.MultivariateNormal(w_mu_prior.flatten(), torch.diag(w_logvar_prior.flatten().exp()))\n",
    "        w_kl = dists.kl.kl_divergence(w_dist, w_prior)\n",
    "        \n",
    "        z_dist = dists.MultivariateNormal(z_mu.flatten(), torch.diag(z_logvar.flatten().exp()))\n",
    "        z_prior = dists.MultivariateNormal(torch.zeros(self.z_dim * z_mu.size()[0]), torch.eye(self.z_dim * z_mu.size()[0]))\n",
    "        z_kl = dists.kl.kl_divergence(z_dist, z_prior)\n",
    "        \n",
    "        y_pred_negentropy = (y_pred.log() * y_pred + (1-y_pred).log() * (1-y_pred)).mean()\n",
    "#         y_pred_negentropy = 0\n",
    "#         print(y_pred_negentropy.item())\n",
    "\n",
    "        y_recon = nn.BCELoss()(y_pred, y)\n",
    "        # alternatively use predicted logvar too to evaluate density of input\n",
    "        \n",
    "        ELBO = 20 * x_recon + 0.2 * z_kl + 1 * w_kl + 10 * y_pred_negentropy + 1 * y_recon\n",
    "        \n",
    "        return ELBO, x_recon, w_kl, z_kl, y_pred_negentropy, y_recon\n",
    "\n",
    "#     def reconstruct_x(self, x, y):\n",
    "#         x_mean, _, _, _, _ = self.forward(x, y)\n",
    "#         return x_mean\n",
    "\n",
    "#     def calculate_nll(self, X, samples=5000):\n",
    "#         \"\"\"\n",
    "#         Estimate NLL by importance sampling\n",
    "#         :param X: dataset, (N, inp_dim)\n",
    "#         :param samples: Samples per observation\n",
    "#         :return: IS estimate\n",
    "#         \"\"\"   \n",
    "#         prob_sum = 0.\n",
    "\n",
    "#         for i in range(samples):\n",
    "#             KL, RE, _ = self.calculate_loss(X)\n",
    "#             prob_sum += (KL + RE).exp_()\n",
    "            \n",
    "#         return - (prob_sum / samples).sum().log_()\n",
    "\n",
    "#     def generate_x(self, N=25):\n",
    "#         \"\"\"\n",
    "#         Sample, using you VAE: sample z from prior and decode it \n",
    "#         :param N: number of samples\n",
    "#         :return: X (N, inp_size)\n",
    "#         \"\"\"\n",
    "\n",
    "#         m = MultivariateNormal(torch.zeros(self.z_dim + self.w_dim), torch.eye(self.z_dim + self.w_dim))\n",
    "#         z = m.sample(sample_shape=torch.Size([N])) \n",
    "        \n",
    "#         X, _ = self.p_x(z.cuda())\n",
    "#         return X\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.FloatTensor(std.size()).normal_().to(mu.device)\n",
    "        return eps.mul(std).add_(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, manifold_x = make_swiss_roll(n_samples=10000)\n",
    "x = x.astype(np.float32)\n",
    "y = (x[:, 0:1] >= 10).astype(np.float32)\n",
    "z_dim = 2\n",
    "w_dim = 2\n",
    "\n",
    "batch_size = 32\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_tensor = torch.from_numpy(x)\n",
    "train_set_y_tensor = torch.from_numpy(y)\n",
    "train_set = utils.TensorDataset(train_set_x_tensor, train_set_y_tensor)\n",
    "train_loader = utils.DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CSVAE(input_dim=x.shape[1], labels_dim=y.shape[1], z_dim=z_dim, w_dim=w_dim)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2300 [00:05<3:33:24,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "MSE(x): 69.6408\n",
      "KL(w): 109.2690\n",
      "KL(z): 20.7010\n",
      "-H(y): -0.6464\n",
      "BCE(y): 20.7010\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2300 [00:10<3:23:14,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "MSE(x): 51.4005\n",
      "KL(w): 42.0397\n",
      "KL(z): 100.2043\n",
      "-H(y): -0.6697\n",
      "BCE(y): 100.2043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2300 [00:14<3:15:55,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "MSE(x): 40.8871\n",
      "KL(w): 47.5004\n",
      "KL(z): 100.5136\n",
      "-H(y): -0.6799\n",
      "BCE(y): 100.5136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2300 [00:20<3:16:30,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "MSE(x): 36.9674\n",
      "KL(w): 45.7177\n",
      "KL(z): 84.1394\n",
      "-H(y): -0.6850\n",
      "BCE(y): 84.1394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2300 [00:25<3:17:15,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "MSE(x): 34.9046\n",
      "KL(w): 44.7872\n",
      "KL(z): 72.3348\n",
      "-H(y): -0.6877\n",
      "BCE(y): 72.3348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2300 [00:30<3:15:16,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "MSE(x): 33.3878\n",
      "KL(w): 44.2110\n",
      "KL(z): 65.2668\n",
      "-H(y): -0.6891\n",
      "BCE(y): 65.2668\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2300 [00:35<3:20:01,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "MSE(x): 32.0832\n",
      "KL(w): 44.5858\n",
      "KL(z): 58.6237\n",
      "-H(y): -0.6898\n",
      "BCE(y): 58.6237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2300 [00:40<3:17:43,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "MSE(x): 31.0290\n",
      "KL(w): 45.5277\n",
      "KL(z): 53.9855\n",
      "-H(y): -0.6902\n",
      "BCE(y): 53.9855\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-25c2868bdde0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcur_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_recon_loss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_kl_loss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_kl_loss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_negentropy_loss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_recon_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcur_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a679680db8dd>\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mw_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_mu_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_logvar_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mw_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_mu_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_logvar_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mw_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprecision_matrix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3/2)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(opt, milestones=[pow(3, i) for i in range(7)], gamma=pow(0.1, 1/7))\n",
    "n_epochs = 2300\n",
    "\n",
    "x_recon_losses = []\n",
    "w_kl_losses = []\n",
    "z_kl_losses = []\n",
    "y_negentropy_losses = []\n",
    "y_recon_losses = []\n",
    "for epoch_i in trange(n_epochs):\n",
    "    for cur_batch in train_loader:\n",
    "        cur_batch = cur_batch\n",
    "        opt.zero_grad()\n",
    "        loss_val, x_recon_loss_val, w_kl_loss_val, z_kl_loss_val, y_negentropy_loss_val, y_recon_loss_val = model.calculate_loss(*cur_batch)\n",
    "        loss_val.backward()\n",
    "        opt.step()\n",
    "        x_recon_losses.append(x_recon_loss_val.item())\n",
    "        w_kl_losses.append(w_kl_loss_val.item())\n",
    "        z_kl_losses.append(z_kl_loss_val.item())\n",
    "        y_negentropy_losses.append(y_negentropy_loss_val.item())\n",
    "        y_recon_losses.append(y_recon_loss_val.item())\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch_i}')\n",
    "    print(f'MSE(x): {np.array(x_recon_losses[-len(train_loader):]).mean():.4f}')\n",
    "    print(f'KL(w): {np.array(w_kl_losses[-len(train_loader):]).mean():.4f}')\n",
    "    print(f'KL(z): {np.array(z_kl_losses[-len(train_loader):]).mean():.4f}')\n",
    "    print(f'-H(y): {np.array(y_negentropy_losses[-len(train_loader):]).mean():.4f}')\n",
    "    print(f'BCE(y): {np.array(z_kl_losses[-len(train_loader):]).mean():.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, manifold_x_test = make_swiss_roll(n_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing process\n",
    "\n",
    "# x_test, manifold_x_test = make_swiss_roll(n_samples=10000)\n",
    "# x_test = x_test.astype(np.float32)\n",
    "# test_set_tensor = torch.from_numpy(x_test)\n",
    "# mu_x, logvar_x, z_hat, mu_z, logvar_z = model.forward(test_set_tensor)\n",
    "\n",
    "# labels_test = (x_test[:, 0:1] >= 10)\n",
    "# colors_test = ['red' if label[0] else 'blue' for label in labels_test]\n",
    "\n",
    "# z_hat = z_hat.detach().numpy()\n",
    "# z_comp = z_hat[:, :2]\n",
    "# w_comp = z_hat[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual VAE results\n",
    "\n",
    "plt.figure(figsize=(5, 5,))\n",
    "plt.title('(z1, z2)')\n",
    "plt.scatter(z_comp[:, 0], z_comp[:, 1], c=colors_test)\n",
    "\n",
    "plt.figure(figsize=(5, 5,))\n",
    "plt.title('(z2, w1)')\n",
    "plt.scatter(z_comp[:, 1], w_comp[:, 0], c=colors_test)\n",
    "\n",
    "plt.figure(figsize=(5, 5,))\n",
    "plt.title('(w1, w2)')\n",
    "plt.scatter(w_comp[:, 0], w_comp[:, 1], c=colors_test)\n",
    "\n",
    "plt.figure(figsize=(5, 5,))\n",
    "plt.title('(w2, z1)')\n",
    "plt.scatter(w_comp[:, 1], w_comp[:, 0], c=colors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
